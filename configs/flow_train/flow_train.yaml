defaults:
  - _self_ # First takes priority so any properties redefined here take precedence
  - model: unet
  - flow: mean
  - paths: ibme # Simply override in cli call

dataset:
  path: ${paths.data_path}/CAMUS_Latents_${vae.resolution}
  normalise_latents: true
  latent_scaling_path: ${paths.data_path}/CAMUS_Latents_${vae.resolution}/CAMUS_Scaling_${vae.resolution}.pt
  max_frames: 32
  batch_size: 5
  cond_pad_mask: true  # Whether to append the padding mask to the conditioning input
  masking_distribution: uniform  # Distribution to use for random masking; 'uniform' or 'geometric'

vae:
  resolution: 16f8 #(keep)
  scaling_factors:
    4f4: 0.1080
    4f8: 0.2232
    16f8: 1.33

  scaling_factor: ${vae.scaling_factors.${vae.resolution}}

trainer:
  lr: 1e-4
  lr_scheduler: cosineannealing
  warmup_epochs: 10
  loss_mask: pad
  uncond_prob: 0.0
  kwargs:
    max_epochs: 1000
    precision: 16
    accelerator: auto
    val_check_interval: 0.1
    accumulate_grad_batches: 3
    #profiler: simple
    #overfit_batches: 0.1 #debug

#ema:
#  kwargs:
#    decay: 0.999
#    start_step: 1
#    every_n_steps: 1
#    use_buffers: true

sample:
  every_n_epochs: 50
  model_sample_kwargs: ${flow.sample_kwargs}


ckpt:
  metric:
    monitor: val_loss
    filename: '{epoch}-{step}-{${ckpt.metric.monitor}:.3f}'
    save_top_k: 1
    mode: min
    save_last: True

  
wandb:
  project: go-with-the-flow
  entity: engemmanuel
  group: ${flow.type}
  #mode: offline

hydra:
  run:
    dir: ${paths.train_output_path}/tests/${now:%Y-%m-%d}/${now:%H-%M-%S}